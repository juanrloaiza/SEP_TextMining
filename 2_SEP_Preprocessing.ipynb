{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing text from the SEP\n",
    "Using the scraper before, we obtained all of the SEP in separate files. From now on, we will consider each of these files as one document. Before we get to run our models, we must first do some preprocessing to make the files friendlier for the machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "files = os.listdir('sep_articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18th_century_french_aesthetics'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing with NLTK\n",
    "The goal is to have a piece of text where all variations of a word are reduced to a single lemma, and where we only have meaningful words. To do this, we must:\n",
    "\n",
    "1. Tokenize the text.\n",
    "    * Tokenizing means breaking the text into pieces, be it words or sentences. In our case, we can use words.\n",
    "3. Part-of-speech tagging.\n",
    "    * This tags each word in terms of its role in the sentence (noun, verb, adjective, etc.)\n",
    "2. Remove stop words.\n",
    "    * Stop words are words that are meaningless (at least for our analysis), such as 'the' or 'and.'\n",
    "4. Lemmatization.\n",
    "    * This reduces word variations to a common lemma (e.g. 'ate' and 'eating' => 'eat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will divide each text into sentences with NLTK's sent_tokenize() function. Then we will divide each sentence into words. We will use the RegExp Tokenizer included in NLTK so that we can have better control on the kinds of words we want to get. For our purposes, we want words without numbers or symbols. We will spell this out in a regular expression when we instantiate the tokenizer. Once we have each sentence as a list of words, we save it to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer('[A-Za-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    with open('sep_articles/' + f) as raw_text:\n",
    "        sent_tokens = nltk.sent_tokenize(raw_text.read().decode('utf8'))\n",
    "    \n",
    "    sent_words = []\n",
    "    for sentence in sent_tokens:\n",
    "        sent_words.append(tokenizer.tokenize(sentence))\n",
    "        \n",
    "    # When we run the tokenizer, some sentences (or at least what the sentence tokenizer thought were sentences) are now empty. \n",
    "    # Let's remove to keep things clean.\n",
    "    sent_words = [sentence for sentence in sent_words if sentence]\n",
    "    \n",
    "    with open('sep_articles_tokenized/' + f[:-4] + '.txt', 'w') as tokenized_text:\n",
    "        tokenized_text.write(str(sent_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging\n",
    "We can now tag each word for its part of speech. It's better to do this before we remove the stop words, as the stop words do contain information about the sentence's syntactic structure, hence about each word's role in the sentence. To do this tagging, we use NLTK's pos_tag() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir('sep_articles_tokenized/'):\n",
    "    with open('sep_articles_tokenized/' + f) as sent_words_raw:\n",
    "        sent_words = ast.literal_eval(sent_words_raw.read())\n",
    "    \n",
    "    sent_tagged = []\n",
    "    for sentence in sent_words:\n",
    "        sent_tagged.append(nltk.pos_tag(sentence))\n",
    "        \n",
    "    with open('sep_articles_tagged/' + f, 'w') as tagged_file:\n",
    "        tagged_file.write(str(sent_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "NLTK includes a list of stopwords we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir('sep_articles_tagged/'):\n",
    "    with open('sep_articles_tagged/' + f) as sent_tagged_raw:\n",
    "        sent_tagged = ast.literal_eval(sent_tagged_raw.read())\n",
    "        \n",
    "    sent_clean = []\n",
    "    for sentence in sent_tagged:\n",
    "        sent_clean.append([word for word in sentence if word[0].lower() not in stop_words])\n",
    "        \n",
    "    with open('sep_articles_clean/' + f, 'w') as clean_file:\n",
    "        clean_file.write(str(sent_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The last pre-processing step is to reduce each word to its lemma. This helps us normalize the count, avoiding counting different variations of the same concept as if they were different words altogether. Again, we do this with the help of NLTK's lemmatizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desired'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('desired', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's lemmatizer considers the part-of-speech of the word in order to get its lemma. This is why we did POS tagging before lemmatization. To make things easier, we build a custom lemmatizer function that just runs the lemmatizer with the corresponding argument for each part of speech. Since we are only interested in nouns, verbs, and adjectives, we will return False for any other part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLemmatizer(tagged_word):\n",
    "    word = tagged_word[0]\n",
    "    pos = tagged_word[1]\n",
    "    \n",
    "    if pos in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    elif pos in ['VB', 'VBN', 'VBZ', 'VBD', 'VBP', 'VBG']:\n",
    "        lemma = lemmatizer.lemmatize(word, pos = 'v')\n",
    "        \n",
    "    elif pos in ['JJ', 'JJR', 'JJS']:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='a')\n",
    "        \n",
    "    else:\n",
    "        lemma = False\n",
    "        \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run the lemmatizer! We will run it for each sentence individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir('sep_articles_clean/'):\n",
    "    with open('sep_articles_clean/' + f) as sent_clean_raw:\n",
    "        sent_clean = ast.literal_eval(sent_clean_raw.read())\n",
    "\n",
    "    lemmatized_text = []\n",
    "    for sentence in sent_clean:\n",
    "        for word in sentence:\n",
    "            lemma = customLemmatizer(word)\n",
    "            if lemma:\n",
    "                lemmatized_text.append(lemma)\n",
    "                \n",
    "    # Remove words with two letters.\n",
    "    lemmatized_text = [word.lower() for word in lemmatized_text if len(word) > 2]\n",
    "                \n",
    "    with open('sep_articles_lemmatized/' + f, 'w') as lemmatized_file:\n",
    "        lemmatized_file.write(str(lemmatized_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
